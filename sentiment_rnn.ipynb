{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U-V8V6olLm9"
      },
      "source": [
        "import torch as T\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs5_QYDqjzo5"
      },
      "source": [
        "DATA_LOADER\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0TLr2zKnaqC"
      },
      "source": [
        "class DATA_LOADER():\n",
        "    def __init__(self):\n",
        "        with open(r'/content/drive/MyDrive/dl_datasets/reviews.txt', 'r') as f:\n",
        "            self.reviews = f.read()\n",
        "\n",
        "        with open(r'/content/drive/MyDrive/dl_datasets/labels.txt', 'r') as f:\n",
        "            self.labels = f.read()\n",
        "\n",
        "        from string import punctuation\n",
        "\n",
        "        self.reviews = self.reviews.lower()\n",
        "        self.reviews = ''.join(\n",
        "            [i for i in self.reviews if i not in punctuation])\n",
        "\n",
        "        # ITS TO BE USED LATER FOR ENCODING THE REVIEWS TO INTEGERS\n",
        "        self.reviews_words = self.reviews.split('\\n')\n",
        "        self.reviews = ' '.join(self.reviews_words)\n",
        "        self.reviews = self.reviews.split()\n",
        "\n",
        "        self.labels = self.labels.split('\\n')\n",
        "        self.labels = [int(i == 'positive') for i in self.labels]\n",
        "\n",
        "        # REVIEWS CONTAINS ALL THE INDIVIDUAL WORDS. WE HAVE TO FIND THE NO. OF OCCURANCES OF THE WORDS.\n",
        "        # SORT THE LIST BASED ON THE MAX OCCURANCES, I.E WORD 'THE' HAS THE HIGHEST OCCURANCES AND HENCE ITS GIVEN INDEX OF 1.\n",
        "        # NOW TAKE THE REVIEW WORDS AND ENCODE THEM WITH THE INDEX VALUE.\n",
        "\n",
        "        from collections import Counter\n",
        "\n",
        "        # COUNT THE NO. OF OCCURANCES OF THE WORDS\n",
        "        self.reviews = Counter(self.reviews)\n",
        "        # SORTING THE WORD LIST BASED ON MAX OCCURANCES\n",
        "        self.words = sorted(self.reviews, key=self.reviews.get, reverse=True)\n",
        "\n",
        "        self.words_to_int = {word: ii for ii, word in enumerate(\n",
        "            self.words, 1)}   # INDEXING THE WORDS IN THE SORTED ORDER\n",
        "        self.encoded_review = []                                             # ENCODED VALUES FOR THE REVIEWS\n",
        "\n",
        "        for review in self.reviews_words:\n",
        "            self.encoded_review.append(\n",
        "                [self.words_to_int[word] for word in review.split()])\n",
        "\n",
        "        # TO REMOVE REVIEWS OF ZERO LENGTH I.E REVIEW AT LAST IS AN EMPTY ARRAY.\n",
        "\n",
        "        len_of_rev = [len(i) for i in self.encoded_review]\n",
        "        max_rev_length = max(len_of_rev)                                # TO FIND THE MAX LENGTH REVIEW\n",
        "\n",
        "        non_zero_idx = [ii for ii, review in enumerate(\n",
        "            self.encoded_review) if len(review) != 0]\n",
        "\n",
        "        self.encoded_review = [self.encoded_review[ii] for ii in non_zero_idx]\n",
        "        self.encoded_labels = np.array(\n",
        "            [self.labels[ii] for ii in non_zero_idx], dtype=np.int64)\n",
        "\n",
        "        # TO FEED TO THE MODEL WE NEED TO TRUNCATE OR EXTEND ALL OUR REVIEWS TO A STD LENGTH LET US TAKE MEAN LENGTH\n",
        "        self.features = self.std_length(self.encoded_review, int(\n",
        "            np.ceil(np.mean(len_of_rev))))\n",
        "        \n",
        "        self.seq_len = int(np.ceil(np.mean(len_of_rev)))\n",
        "\n",
        "    def std_length(self, encoded_review, seq_len):\n",
        "        features = np.zeros((len(encoded_review), seq_len), dtype=np.int64)\n",
        "        for i, j in enumerate(encoded_review):\n",
        "            ind = min(len(j), seq_len)\n",
        "            features[i, -ind:] = j[:ind]\n",
        "        return features\n",
        "\n",
        "        \n",
        "        #print(features.shape)\n",
        "        #print(features[0], labels[0])\n",
        "\n",
        "    def sentiment_dataloader(self):\n",
        "        train_split = int(len(self.features) * 0.8)\n",
        "        val_split = int(len(self.features) * 0.9)\n",
        "\n",
        "        train_features, val_features, test_features = self.features[:train_split], \\\n",
        "                                                self.features[train_split:val_split], self.features[val_split:]\n",
        "\n",
        "        train_labels, val_labels, test_labels = self.encoded_labels[:train_split], \\\n",
        "            self.encoded_labels[train_split:val_split], self.encoded_labels[val_split:]\n",
        "\n",
        "        train_data = TensorDataset(T.from_numpy(train_features), T.from_numpy(train_labels))\n",
        "        val_data = TensorDataset(T.from_numpy(val_features), T.from_numpy(val_labels))\n",
        "        test_data = TensorDataset(T.from_numpy(test_features), T.from_numpy(test_labels))\n",
        "\n",
        "        train_loader = DataLoader(train_data, batch_size=50, shuffle=True)\n",
        "        val_loader = DataLoader(val_data, batch_size=50, shuffle=True)\n",
        "        test_loader = DataLoader(test_data, batch_size=50, shuffle=True)\n",
        "\n",
        "        return train_loader, val_loader, test_loader"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQyu4fODj-X2"
      },
      "source": [
        "MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC_dkLq4kATo"
      },
      "source": [
        "class LSTM_SENTIMENT(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hid_dims, n_layers, out_dims):\n",
        "        super().__init__()\n",
        "        self.hid_dims = hid_dims\n",
        "        self.out_dims = out_dims\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.drop_prob = 0.5\n",
        "        self.lr = 0.001\n",
        "        self.device = 'cuda' if T.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, self.hid_dims, n_layers,\n",
        "                            dropout=self.drop_prob, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(self.drop_prob)\n",
        "\n",
        "        self.fc = nn.Linear(self.hid_dims, self.out_dims)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x, h):\n",
        "        \n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        embed = self.embedding(x.long()).to(self.device)\n",
        "        #print(h[0].shape)\n",
        "        o, h = self.lstm(embed, h)\n",
        "        \n",
        "        o = o[:,-1,:]\n",
        "        \n",
        "        o = self.dropout(o)\n",
        "        \n",
        "        o = self.sigmoid(self.fc(o))\n",
        "        \n",
        "        return o, h\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        \n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        hidden = (weight.new_zeros(self.n_layers, batch_size, self.hid_dims).to(self.device),\n",
        "                  weight.new_zeros(self.n_layers, batch_size, self.hid_dims).to(self.device))\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rz_bfvHkEjy"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP0oJVjyouf-"
      },
      "source": [
        "dat = DATA_LOADER()\n",
        "train, val, test = dat.sentiment_dataloader()\n",
        "vocab_size = len(dat.words_to_int) + 1\n",
        "\n",
        "out_dim = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "batch_size = 50\n",
        "lstm = LSTM_SENTIMENT(vocab_size, embedding_dim,\n",
        "                      hidden_dim, n_layers, out_dim)\n",
        "lstm.to(lstm.device)\n",
        "#print(lstm)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = Adam(lstm.parameters(), lr = 0.001)\n",
        "\n",
        "ctr = 0\n",
        "\n",
        "print_every = 100"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f42lCKvHkGMv",
        "outputId": "c403d9b6-0ce3-440e-f09f-84638a934f7d"
      },
      "source": [
        "for e in range(4):\n",
        "    \n",
        "    hidden = lstm.init_hidden(batch_size)\n",
        "\n",
        "    for inp, tgt in train:\n",
        "        lstm.train()\n",
        "        ctr += 1\n",
        "        hidden = tuple([x.data for x in hidden]) \n",
        "\n",
        "        lstm.zero_grad()\n",
        "\n",
        "        inp, tgt = inp.to(lstm.device), tgt.to(lstm.device)\n",
        "        \n",
        "        #print(hidden[0].shape, hidden[1].shape)\n",
        "        out, hidden = lstm(inp, hidden)\n",
        "\n",
        "        loss = loss_fn(out.squeeze(), tgt.float())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(lstm.parameters(), 5)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if ctr % print_every == 0:\n",
        "            val_loss = []\n",
        "\n",
        "            val_h = lstm.init_hidden(batch_size)\n",
        "            \n",
        "            lstm.eval()\n",
        "\n",
        "            for val_x, val_y in val:\n",
        "                \n",
        "                val_h = tuple([x.data for x in val_h])\n",
        "\n",
        "                val_x, val_y = val_x.to(lstm.device), val_y.to(lstm.device)\n",
        "\n",
        "                val_out, val_h = lstm(val_x, val_h)\n",
        "\n",
        "                val_l = loss_fn(val_out.squeeze(), val_y.float())\n",
        "                val_loss.append(val_l.item())\n",
        "            \n",
        "            print(f'\\\n",
        "                EPOCH : {e:>2}\\\n",
        "                STEP :  {ctr:>2}\\\n",
        "                VAL_LOSS : {np.mean(val_loss):>8.6f}\\\n",
        "                LOSS : {loss.item():>8.6f}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                EPOCH :  0                STEP :  100                VAL_LOSS : 0.649618                LOSS : 0.647455\n",
            "                EPOCH :  0                STEP :  200                VAL_LOSS : 0.576907                LOSS : 0.654679\n",
            "                EPOCH :  0                STEP :  300                VAL_LOSS : 0.595175                LOSS : 0.558573\n",
            "                EPOCH :  0                STEP :  400                VAL_LOSS : 0.569343                LOSS : 0.562032\n",
            "                EPOCH :  1                STEP :  500                VAL_LOSS : 0.494353                LOSS : 0.392687\n",
            "                EPOCH :  1                STEP :  600                VAL_LOSS : 0.482486                LOSS : 0.286285\n",
            "                EPOCH :  1                STEP :  700                VAL_LOSS : 0.505159                LOSS : 0.367636\n",
            "                EPOCH :  1                STEP :  800                VAL_LOSS : 0.453432                LOSS : 0.378100\n",
            "                EPOCH :  2                STEP :  900                VAL_LOSS : 0.456390                LOSS : 0.299851\n",
            "                EPOCH :  2                STEP :  1000                VAL_LOSS : 0.460222                LOSS : 0.318740\n",
            "                EPOCH :  2                STEP :  1100                VAL_LOSS : 0.414371                LOSS : 0.299829\n",
            "                EPOCH :  2                STEP :  1200                VAL_LOSS : 0.413598                LOSS : 0.250447\n",
            "                EPOCH :  3                STEP :  1300                VAL_LOSS : 0.449849                LOSS : 0.101937\n",
            "                EPOCH :  3                STEP :  1400                VAL_LOSS : 0.506805                LOSS : 0.123537\n",
            "                EPOCH :  3                STEP :  1500                VAL_LOSS : 0.448609                LOSS : 0.253042\n",
            "                EPOCH :  3                STEP :  1600                VAL_LOSS : 0.476813                LOSS : 0.085892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXeVrAqhuSdb"
      },
      "source": [
        "TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rya1mL6uuUun",
        "outputId": "f43bc18b-30cc-4f40-dfbc-4998c72e8049"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "\n",
        "h = lstm.init_hidden(batch_size)\n",
        "\n",
        "lstm.eval()\n",
        "\n",
        "for inputs, labels in test:\n",
        "\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    inputs, labels = inputs.to(lstm.device), labels.to(lstm.device)\n",
        "    \n",
        "    output, h = lstm(inputs, h)\n",
        "\n",
        "    test_loss = loss_fn(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    pred = T.round(output.squeeze())\n",
        "\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy()) \n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "test_acc = num_correct/len(test.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.458\n",
            "Test accuracy: 0.821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT4hqoMyseKU"
      },
      "source": [
        "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k7n-EOurRiU"
      },
      "source": [
        "from string import punctuation\n",
        "def tokenize_review(test_review):\n",
        "    test_review = test_review.lower()\n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "    test_words = test_text.split()\n",
        "    test_ints = []\n",
        "    test_ints.append([dat.words_to_int.get(word, 0) for word in test_words])\n",
        "    return test_ints"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_dEsr7DvNMc"
      },
      "source": [
        "def predict(model, test_review, sequence_length=241):\n",
        "\n",
        "    model.eval()\n",
        "    test_ints = tokenize_review(test_review)\n",
        "\n",
        "    features = dat.std_length(test_ints, sequence_length)\n",
        "    feature_tensor = T.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.shape[0]\n",
        "\n",
        "    h = model.init_hidden(batch_size)\n",
        "  \n",
        "    feature_tensor = feature_tensor.to(model.device)\n",
        "\n",
        "    output, h = model(feature_tensor, h)\n",
        "\n",
        "    pred = T.round(output.squeeze()) \n",
        "    \n",
        "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "\n",
        "    if(pred.item()==1):\n",
        "        print(\"Positive review!\")\n",
        "    else:\n",
        "        print(\"Negative review\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrZmEJyuvyFA",
        "outputId": "d65b3b4f-f2e7-4bf7-c001-1a3e50cdb39b"
      },
      "source": [
        "predict(lstm, test_review_neg, dat.seq_len)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction value, pre-rounding: 0.487199\n",
            "Negative review\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}