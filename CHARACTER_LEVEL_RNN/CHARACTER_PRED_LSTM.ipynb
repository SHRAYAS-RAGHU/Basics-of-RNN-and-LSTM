{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CHARACTER_PRED_LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qwh-jcxnPjf"
      },
      "source": [
        "import torch as T\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBbF-rAWnayr"
      },
      "source": [
        "class DATA_LOADER():\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        with open(self.filename, 'r') as f:\n",
        "            text_data = f.read()\n",
        "\n",
        "        # TOKENISATION FOR DATA ENCODING\n",
        "        self.char_in_text = tuple(set(text_data))\n",
        "        self.n_characters = len(self.char_in_text)\n",
        "\n",
        "        # DICT FOR KEY AS NOS. AND VALUE AS CHARACTER\n",
        "        self.I2C = dict(enumerate(self.char_in_text))\n",
        "        self.C2I = {v: k for k, v in self.I2C.items()}\n",
        "\n",
        "        # ENCODING THE GIVEN TEXT\n",
        "        self.encoded_text = np.array([self.C2I[i] for i in text_data])\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        RETURNS ENCODED TEXT AND LENGTH OF SEQ\n",
        "        \"\"\"\n",
        "        return self.encoded_text, self.n_characters\n",
        "\n",
        "\n",
        "    def one_hot(self, inp, length):\n",
        "        a = np.zeros((inp.size, length))\n",
        "        a[np.arange(a.shape[0]), inp.flatten()] = 1\n",
        "        a = a.reshape((*inp.shape, length))\n",
        "        return a\n",
        "\n",
        "    #encoded_text, no_of_char = load_data(r'C:\\Users\\krshr\\Desktop\\Files\\Deep_learning\\NLP_RNN_LSTM\\CHARACTER_LEVEL_RNN\\data.txt')\n",
        "    #encoded_text = one_hot(encoded_text, no_of_char)\n",
        "\n",
        "    def get_batch(self, a, BATCH_SIZE, SEQ_LENGTH):\n",
        "\n",
        "        PER_BATCH_ELEMENTS = BATCH_SIZE * SEQ_LENGTH    # 400\n",
        "        N = len(a) // PER_BATCH_ELEMENTS                # 3854\n",
        "\n",
        "        a = a[:N * PER_BATCH_ELEMENTS]                  # (1541600,)\n",
        "        a = a.reshape((BATCH_SIZE, -1))                 # (8, 192700)\n",
        "\n",
        "        for i in range(0, a.shape[1], SEQ_LENGTH):\n",
        "            \n",
        "            x = a[:, i : i+SEQ_LENGTH]\n",
        "            y = np.zeros_like(x)\n",
        "\n",
        "            try:\n",
        "                y[:, :-1], y[:, -1] = x[:, 1:], x[:, N + SEQ_LENGTH]\n",
        "            except:\n",
        "                y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
        "            \n",
        "            yield x, y"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmjWVPoBoTaN"
      },
      "source": [
        "class  LSTM(nn.Module):\n",
        "    def __init__(self, in_dims, hid_dims, n_layers, out_dims):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_dims = in_dims\n",
        "        self.hid_dims = hid_dims\n",
        "        self.n_layers = n_layers\n",
        "        self.out_dims = out_dims\n",
        "        self.drop_prob=0.5\n",
        "        self.lr=0.001\n",
        "        self.device = 'cuda' if T.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.LSTM = nn.LSTM(self.in_dims, self.hid_dims, n_layers,\n",
        "                            dropout=self.drop_prob, batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(self.drop_prob)\n",
        "\n",
        "        self.fc = nn.Linear(self.hid_dims, self.out_dims)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \n",
        "        out, hidden = self.LSTM(x, hidden)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = out.contiguous().view(-1, self.hid_dims)\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hid_dims).zero_().to(self.device),\n",
        "                  weight.new(self.n_layers, batch_size, self.hid_dims).zero_().to(self.device))\n",
        "                \n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuFwnyDfbhrA"
      },
      "source": [
        "def train(data_loader, net_object, dataset, epochs=10, batch_size=8, \n",
        "                seq_length=50, lr=0.001, clip_grad_value=5, val_frac=0.1, print_every=10, device = 'cpu'):\n",
        "    net_object.train()\n",
        "\n",
        "    data, no_of_char = dataset\n",
        "\n",
        "    optimizer = Adam(net_object.parameters(), lr = lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # VALIDATION SPLIT OF 0.1 CAN BE DONE BY FINDING THE VAL_INDEX AND SPLITTING IT\n",
        "    val_index = int(len(data) * (1 - val_frac))\n",
        "    train_data, val_data = data[:val_index], data[val_index:]\n",
        "\n",
        "    ctr = 0\n",
        "    min_val_loss = 10\n",
        "    for e in range(epochs):\n",
        "        \n",
        "        # CREATES HIDDEN LAYER (H, C) FOR LTM AND STM\n",
        "        h = net_object.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in data_loader.get_batch(train_data, batch_size, seq_length):\n",
        "            ctr += 1\n",
        "            h = tuple([hh.data for hh in h])\n",
        "            net_object.zero_grad()\n",
        "            x = data_loader.one_hot(x, no_of_char)\n",
        "            x = T.Tensor(x).to(device)\n",
        "            # PYTORCH EXPECTS TARGET OF TYPE LONG FOR CE-LOSS\n",
        "            y = T.Tensor(y).view(batch_size*seq_length).long().to(device)\n",
        "            out, h = net_object(x, h)\n",
        "            #print(out.shape, y.shape, hidden[0].shape, hidden[1].shape)\n",
        "            loss = loss_fn(out, y)\n",
        "            loss.backward()\n",
        "\n",
        "            # TO PREVENT THE PROBLEM OF EXPLODING GRADIENTS WE CLIP THE GRADIENTS\n",
        "            nn.utils.clip_grad.clip_grad_norm_(net_object.parameters(), clip_grad_value)\n",
        "            optimizer.step()\n",
        "            \n",
        "            if ctr % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net_object.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net_object.eval() # NETWORK SET FOR EVALUVATION TO STOP BACK PROP\n",
        "                for x, y in data_loader.get_batch(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = data_loader.one_hot(x, no_of_char)\n",
        "                    x, y = T.Tensor(x), T.Tensor(y).view(batch_size*seq_length).long()\n",
        "\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x.to(device), y.to(device)\n",
        "                    \n",
        "\n",
        "                    output, val_h = net_object(inputs, val_h)\n",
        "                    val_loss = loss_fn(output, targets)\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net_object.train()  \n",
        "\n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(ctr),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "                min_val_loss = min(val_loss, min_val_loss)\n",
        "                if val_loss < loss and val_loss <= min_val_loss:\n",
        "                    T.save(net_object.state_dict(), r'/content/drive/MyDrive/CHARACTER_LEVEL_RNN/checkpoint.pth')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RANTgJoooZT_",
        "outputId": "f3dda4bb-da4c-45d4-a418-f5274dbf4ebb"
      },
      "source": [
        "data_loader = DATA_LOADER(r'/content/drive/MyDrive/CHARACTER_LEVEL_RNN/data.txt')\n",
        "encoded_text, no_of_char = data_loader.load_data()\n",
        "\n",
        "lstm = LSTM(no_of_char, 512, 2, no_of_char)\n",
        "lstm.to(lstm.device)\n",
        "#print(lstm)\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20  # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(data_loader, lstm, (encoded_text, no_of_char), epochs=n_epochs, batch_size=batch_size,\n",
        "      seq_length=seq_length, lr=0.001, print_every=10, device='cuda')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2653... Val Loss: 3.2310\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1762... Val Loss: 3.1426\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1483... Val Loss: 3.1291\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1504... Val Loss: 3.1228\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1297... Val Loss: 3.1217\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1138... Val Loss: 3.1208\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1102... Val Loss: 3.1182\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1128... Val Loss: 3.1155\n",
            "Epoch: 1/20... Step: 90... Loss: 3.0846... Val Loss: 3.1099\n",
            "Epoch: 1/20... Step: 100... Loss: 3.0915... Val Loss: 3.0965\n",
            "Epoch: 2/20... Step: 110... Loss: 3.0608... Val Loss: 3.0646\n",
            "Epoch: 2/20... Step: 120... Loss: 2.9805... Val Loss: 2.9918\n",
            "Epoch: 2/20... Step: 130... Loss: 2.9263... Val Loss: 2.9178\n",
            "Epoch: 2/20... Step: 140... Loss: 2.8005... Val Loss: 2.7771\n",
            "Epoch: 2/20... Step: 150... Loss: 2.7759... Val Loss: 2.7168\n",
            "Epoch: 2/20... Step: 160... Loss: 2.6518... Val Loss: 2.6202\n",
            "Epoch: 2/20... Step: 170... Loss: 2.5893... Val Loss: 2.5506\n",
            "Epoch: 2/20... Step: 180... Loss: 2.5174... Val Loss: 2.4999\n",
            "Epoch: 2/20... Step: 190... Loss: 2.4791... Val Loss: 2.4608\n",
            "Epoch: 2/20... Step: 200... Loss: 2.4443... Val Loss: 2.4251\n",
            "Epoch: 2/20... Step: 210... Loss: 2.4429... Val Loss: 2.3982\n",
            "Epoch: 3/20... Step: 220... Loss: 2.3960... Val Loss: 2.3634\n",
            "Epoch: 3/20... Step: 230... Loss: 2.3661... Val Loss: 2.3336\n",
            "Epoch: 3/20... Step: 240... Loss: 2.3345... Val Loss: 2.3062\n",
            "Epoch: 3/20... Step: 250... Loss: 2.2970... Val Loss: 2.2781\n",
            "Epoch: 3/20... Step: 260... Loss: 2.3228... Val Loss: 2.2500\n",
            "Epoch: 3/20... Step: 270... Loss: 2.2728... Val Loss: 2.2247\n",
            "Epoch: 3/20... Step: 280... Loss: 2.2482... Val Loss: 2.2007\n",
            "Epoch: 3/20... Step: 290... Loss: 2.2330... Val Loss: 2.1723\n",
            "Epoch: 3/20... Step: 300... Loss: 2.1900... Val Loss: 2.1526\n",
            "Epoch: 3/20... Step: 310... Loss: 2.1471... Val Loss: 2.1305\n",
            "Epoch: 3/20... Step: 320... Loss: 2.1896... Val Loss: 2.1196\n",
            "Epoch: 4/20... Step: 330... Loss: 2.1451... Val Loss: 2.1009\n",
            "Epoch: 4/20... Step: 340... Loss: 2.1562... Val Loss: 2.0703\n",
            "Epoch: 4/20... Step: 350... Loss: 2.1236... Val Loss: 2.0526\n",
            "Epoch: 4/20... Step: 360... Loss: 2.0640... Val Loss: 2.0352\n",
            "Epoch: 4/20... Step: 370... Loss: 2.0682... Val Loss: 2.0187\n",
            "Epoch: 4/20... Step: 380... Loss: 2.0364... Val Loss: 2.0035\n",
            "Epoch: 4/20... Step: 390... Loss: 2.0755... Val Loss: 1.9844\n",
            "Epoch: 4/20... Step: 400... Loss: 2.0470... Val Loss: 1.9734\n",
            "Epoch: 4/20... Step: 410... Loss: 1.9933... Val Loss: 1.9536\n",
            "Epoch: 4/20... Step: 420... Loss: 2.0025... Val Loss: 1.9378\n",
            "Epoch: 4/20... Step: 430... Loss: 2.0004... Val Loss: 1.9249\n",
            "Epoch: 5/20... Step: 440... Loss: 1.9599... Val Loss: 1.9098\n",
            "Epoch: 5/20... Step: 450... Loss: 1.9451... Val Loss: 1.8967\n",
            "Epoch: 5/20... Step: 460... Loss: 1.9419... Val Loss: 1.8859\n",
            "Epoch: 5/20... Step: 470... Loss: 1.9568... Val Loss: 1.8730\n",
            "Epoch: 5/20... Step: 480... Loss: 1.9299... Val Loss: 1.8619\n",
            "Epoch: 5/20... Step: 490... Loss: 1.8894... Val Loss: 1.8441\n",
            "Epoch: 5/20... Step: 500... Loss: 1.8945... Val Loss: 1.8346\n",
            "Epoch: 5/20... Step: 510... Loss: 1.8929... Val Loss: 1.8214\n",
            "Epoch: 5/20... Step: 520... Loss: 1.9011... Val Loss: 1.8122\n",
            "Epoch: 5/20... Step: 530... Loss: 1.8439... Val Loss: 1.8001\n",
            "Epoch: 5/20... Step: 540... Loss: 1.8751... Val Loss: 1.7925\n",
            "Epoch: 6/20... Step: 550... Loss: 1.8265... Val Loss: 1.7822\n",
            "Epoch: 6/20... Step: 560... Loss: 1.8292... Val Loss: 1.7705\n",
            "Epoch: 6/20... Step: 570... Loss: 1.8373... Val Loss: 1.7618\n",
            "Epoch: 6/20... Step: 580... Loss: 1.8398... Val Loss: 1.7536\n",
            "Epoch: 6/20... Step: 590... Loss: 1.8061... Val Loss: 1.7451\n",
            "Epoch: 6/20... Step: 600... Loss: 1.7914... Val Loss: 1.7350\n",
            "Epoch: 6/20... Step: 610... Loss: 1.7899... Val Loss: 1.7254\n",
            "Epoch: 6/20... Step: 620... Loss: 1.7633... Val Loss: 1.7184\n",
            "Epoch: 6/20... Step: 630... Loss: 1.7929... Val Loss: 1.7151\n",
            "Epoch: 6/20... Step: 640... Loss: 1.7501... Val Loss: 1.7051\n",
            "Epoch: 7/20... Step: 650... Loss: 1.8117... Val Loss: 1.6983\n",
            "Epoch: 7/20... Step: 660... Loss: 1.7420... Val Loss: 1.6887\n",
            "Epoch: 7/20... Step: 670... Loss: 1.7386... Val Loss: 1.6805\n",
            "Epoch: 7/20... Step: 680... Loss: 1.7523... Val Loss: 1.6716\n",
            "Epoch: 7/20... Step: 690... Loss: 1.7372... Val Loss: 1.6659\n",
            "Epoch: 7/20... Step: 700... Loss: 1.7527... Val Loss: 1.6602\n",
            "Epoch: 7/20... Step: 710... Loss: 1.7222... Val Loss: 1.6566\n",
            "Epoch: 7/20... Step: 720... Loss: 1.6947... Val Loss: 1.6473\n",
            "Epoch: 7/20... Step: 730... Loss: 1.7342... Val Loss: 1.6454\n",
            "Epoch: 7/20... Step: 740... Loss: 1.6998... Val Loss: 1.6389\n",
            "Epoch: 7/20... Step: 750... Loss: 1.6735... Val Loss: 1.6316\n",
            "Epoch: 8/20... Step: 760... Loss: 1.6802... Val Loss: 1.6265\n",
            "Epoch: 8/20... Step: 770... Loss: 1.6847... Val Loss: 1.6180\n",
            "Epoch: 8/20... Step: 780... Loss: 1.6686... Val Loss: 1.6140\n",
            "Epoch: 8/20... Step: 790... Loss: 1.6687... Val Loss: 1.6059\n",
            "Epoch: 8/20... Step: 800... Loss: 1.6664... Val Loss: 1.6066\n",
            "Epoch: 8/20... Step: 810... Loss: 1.6831... Val Loss: 1.5979\n",
            "Epoch: 8/20... Step: 820... Loss: 1.6594... Val Loss: 1.5958\n",
            "Epoch: 8/20... Step: 830... Loss: 1.6540... Val Loss: 1.5877\n",
            "Epoch: 8/20... Step: 840... Loss: 1.6369... Val Loss: 1.5839\n",
            "Epoch: 8/20... Step: 850... Loss: 1.6011... Val Loss: 1.5848\n",
            "Epoch: 8/20... Step: 860... Loss: 1.6669... Val Loss: 1.5742\n",
            "Epoch: 9/20... Step: 870... Loss: 1.6363... Val Loss: 1.5715\n",
            "Epoch: 9/20... Step: 880... Loss: 1.6578... Val Loss: 1.5674\n",
            "Epoch: 9/20... Step: 890... Loss: 1.6342... Val Loss: 1.5618\n",
            "Epoch: 9/20... Step: 900... Loss: 1.5912... Val Loss: 1.5577\n",
            "Epoch: 9/20... Step: 910... Loss: 1.5996... Val Loss: 1.5549\n",
            "Epoch: 9/20... Step: 920... Loss: 1.6038... Val Loss: 1.5498\n",
            "Epoch: 9/20... Step: 930... Loss: 1.6283... Val Loss: 1.5443\n",
            "Epoch: 9/20... Step: 940... Loss: 1.6200... Val Loss: 1.5422\n",
            "Epoch: 9/20... Step: 950... Loss: 1.5949... Val Loss: 1.5404\n",
            "Epoch: 9/20... Step: 960... Loss: 1.5888... Val Loss: 1.5372\n",
            "Epoch: 9/20... Step: 970... Loss: 1.6041... Val Loss: 1.5318\n",
            "Epoch: 10/20... Step: 980... Loss: 1.5855... Val Loss: 1.5261\n",
            "Epoch: 10/20... Step: 990... Loss: 1.5667... Val Loss: 1.5237\n",
            "Epoch: 10/20... Step: 1000... Loss: 1.5759... Val Loss: 1.5209\n",
            "Epoch: 10/20... Step: 1010... Loss: 1.5777... Val Loss: 1.5178\n",
            "Epoch: 10/20... Step: 1020... Loss: 1.5623... Val Loss: 1.5123\n",
            "Epoch: 10/20... Step: 1030... Loss: 1.5335... Val Loss: 1.5089\n",
            "Epoch: 10/20... Step: 1040... Loss: 1.5534... Val Loss: 1.5047\n",
            "Epoch: 10/20... Step: 1050... Loss: 1.5695... Val Loss: 1.5038\n",
            "Epoch: 10/20... Step: 1060... Loss: 1.5791... Val Loss: 1.5017\n",
            "Epoch: 10/20... Step: 1070... Loss: 1.5324... Val Loss: 1.4996\n",
            "Epoch: 10/20... Step: 1080... Loss: 1.5671... Val Loss: 1.4975\n",
            "Epoch: 11/20... Step: 1090... Loss: 1.5401... Val Loss: 1.4918\n",
            "Epoch: 11/20... Step: 1100... Loss: 1.5274... Val Loss: 1.4871\n",
            "Epoch: 11/20... Step: 1110... Loss: 1.5556... Val Loss: 1.4877\n",
            "Epoch: 11/20... Step: 1120... Loss: 1.5510... Val Loss: 1.4837\n",
            "Epoch: 11/20... Step: 1130... Loss: 1.5303... Val Loss: 1.4804\n",
            "Epoch: 11/20... Step: 1140... Loss: 1.5093... Val Loss: 1.4774\n",
            "Epoch: 11/20... Step: 1150... Loss: 1.5051... Val Loss: 1.4758\n",
            "Epoch: 11/20... Step: 1160... Loss: 1.5023... Val Loss: 1.4719\n",
            "Epoch: 11/20... Step: 1170... Loss: 1.5452... Val Loss: 1.4704\n",
            "Epoch: 11/20... Step: 1180... Loss: 1.4958... Val Loss: 1.4695\n",
            "Epoch: 12/20... Step: 1190... Loss: 1.5584... Val Loss: 1.4639\n",
            "Epoch: 12/20... Step: 1200... Loss: 1.4941... Val Loss: 1.4587\n",
            "Epoch: 12/20... Step: 1210... Loss: 1.4891... Val Loss: 1.4570\n",
            "Epoch: 12/20... Step: 1220... Loss: 1.5064... Val Loss: 1.4555\n",
            "Epoch: 12/20... Step: 1230... Loss: 1.5025... Val Loss: 1.4538\n",
            "Epoch: 12/20... Step: 1240... Loss: 1.5049... Val Loss: 1.4512\n",
            "Epoch: 12/20... Step: 1250... Loss: 1.4902... Val Loss: 1.4474\n",
            "Epoch: 12/20... Step: 1260... Loss: 1.4686... Val Loss: 1.4457\n",
            "Epoch: 12/20... Step: 1270... Loss: 1.5218... Val Loss: 1.4441\n",
            "Epoch: 12/20... Step: 1280... Loss: 1.4859... Val Loss: 1.4438\n",
            "Epoch: 12/20... Step: 1290... Loss: 1.4532... Val Loss: 1.4402\n",
            "Epoch: 13/20... Step: 1300... Loss: 1.4782... Val Loss: 1.4381\n",
            "Epoch: 13/20... Step: 1310... Loss: 1.4661... Val Loss: 1.4349\n",
            "Epoch: 13/20... Step: 1320... Loss: 1.4682... Val Loss: 1.4332\n",
            "Epoch: 13/20... Step: 1330... Loss: 1.4832... Val Loss: 1.4290\n",
            "Epoch: 13/20... Step: 1340... Loss: 1.4681... Val Loss: 1.4290\n",
            "Epoch: 13/20... Step: 1350... Loss: 1.4932... Val Loss: 1.4281\n",
            "Epoch: 13/20... Step: 1360... Loss: 1.4665... Val Loss: 1.4227\n",
            "Epoch: 13/20... Step: 1370... Loss: 1.4567... Val Loss: 1.4238\n",
            "Epoch: 13/20... Step: 1380... Loss: 1.4620... Val Loss: 1.4206\n",
            "Epoch: 13/20... Step: 1390... Loss: 1.4222... Val Loss: 1.4206\n",
            "Epoch: 13/20... Step: 1400... Loss: 1.4686... Val Loss: 1.4165\n",
            "Epoch: 14/20... Step: 1410... Loss: 1.4586... Val Loss: 1.4148\n",
            "Epoch: 14/20... Step: 1420... Loss: 1.4750... Val Loss: 1.4130\n",
            "Epoch: 14/20... Step: 1430... Loss: 1.4506... Val Loss: 1.4105\n",
            "Epoch: 14/20... Step: 1440... Loss: 1.4305... Val Loss: 1.4098\n",
            "Epoch: 14/20... Step: 1450... Loss: 1.4231... Val Loss: 1.4093\n",
            "Epoch: 14/20... Step: 1460... Loss: 1.4435... Val Loss: 1.4054\n",
            "Epoch: 14/20... Step: 1470... Loss: 1.4591... Val Loss: 1.4025\n",
            "Epoch: 14/20... Step: 1480... Loss: 1.4645... Val Loss: 1.4019\n",
            "Epoch: 14/20... Step: 1490... Loss: 1.4387... Val Loss: 1.4022\n",
            "Epoch: 14/20... Step: 1500... Loss: 1.4246... Val Loss: 1.4030\n",
            "Epoch: 14/20... Step: 1510... Loss: 1.4456... Val Loss: 1.3974\n",
            "Epoch: 15/20... Step: 1520... Loss: 1.4250... Val Loss: 1.3950\n",
            "Epoch: 15/20... Step: 1530... Loss: 1.4213... Val Loss: 1.3945\n",
            "Epoch: 15/20... Step: 1540... Loss: 1.4231... Val Loss: 1.3937\n",
            "Epoch: 15/20... Step: 1550... Loss: 1.4362... Val Loss: 1.3945\n",
            "Epoch: 15/20... Step: 1560... Loss: 1.4205... Val Loss: 1.3908\n",
            "Epoch: 15/20... Step: 1570... Loss: 1.3868... Val Loss: 1.3893\n",
            "Epoch: 15/20... Step: 1580... Loss: 1.4016... Val Loss: 1.3839\n",
            "Epoch: 15/20... Step: 1590... Loss: 1.4403... Val Loss: 1.3856\n",
            "Epoch: 15/20... Step: 1600... Loss: 1.4356... Val Loss: 1.3855\n",
            "Epoch: 15/20... Step: 1610... Loss: 1.3980... Val Loss: 1.3835\n",
            "Epoch: 15/20... Step: 1620... Loss: 1.4268... Val Loss: 1.3804\n",
            "Epoch: 16/20... Step: 1630... Loss: 1.3912... Val Loss: 1.3820\n",
            "Epoch: 16/20... Step: 1640... Loss: 1.3952... Val Loss: 1.3787\n",
            "Epoch: 16/20... Step: 1650... Loss: 1.4203... Val Loss: 1.3766\n",
            "Epoch: 16/20... Step: 1660... Loss: 1.4236... Val Loss: 1.3775\n",
            "Epoch: 16/20... Step: 1670... Loss: 1.3971... Val Loss: 1.3767\n",
            "Epoch: 16/20... Step: 1680... Loss: 1.3905... Val Loss: 1.3747\n",
            "Epoch: 16/20... Step: 1690... Loss: 1.3842... Val Loss: 1.3696\n",
            "Epoch: 16/20... Step: 1700... Loss: 1.3775... Val Loss: 1.3705\n",
            "Epoch: 16/20... Step: 1710... Loss: 1.4317... Val Loss: 1.3701\n",
            "Epoch: 16/20... Step: 1720... Loss: 1.3765... Val Loss: 1.3690\n",
            "Epoch: 17/20... Step: 1730... Loss: 1.4274... Val Loss: 1.3667\n",
            "Epoch: 17/20... Step: 1740... Loss: 1.3801... Val Loss: 1.3665\n",
            "Epoch: 17/20... Step: 1750... Loss: 1.3703... Val Loss: 1.3622\n",
            "Epoch: 17/20... Step: 1760... Loss: 1.3939... Val Loss: 1.3628\n",
            "Epoch: 17/20... Step: 1770... Loss: 1.3981... Val Loss: 1.3621\n",
            "Epoch: 17/20... Step: 1780... Loss: 1.3901... Val Loss: 1.3601\n",
            "Epoch: 17/20... Step: 1790... Loss: 1.3778... Val Loss: 1.3593\n",
            "Epoch: 17/20... Step: 1800... Loss: 1.3615... Val Loss: 1.3563\n",
            "Epoch: 17/20... Step: 1810... Loss: 1.4178... Val Loss: 1.3577\n",
            "Epoch: 17/20... Step: 1820... Loss: 1.3794... Val Loss: 1.3562\n",
            "Epoch: 17/20... Step: 1830... Loss: 1.3555... Val Loss: 1.3555\n",
            "Epoch: 18/20... Step: 1840... Loss: 1.3632... Val Loss: 1.3534\n",
            "Epoch: 18/20... Step: 1850... Loss: 1.3592... Val Loss: 1.3524\n",
            "Epoch: 18/20... Step: 1860... Loss: 1.3666... Val Loss: 1.3527\n",
            "Epoch: 18/20... Step: 1870... Loss: 1.3814... Val Loss: 1.3500\n",
            "Epoch: 18/20... Step: 1880... Loss: 1.3570... Val Loss: 1.3493\n",
            "Epoch: 18/20... Step: 1890... Loss: 1.3777... Val Loss: 1.3496\n",
            "Epoch: 18/20... Step: 1900... Loss: 1.3673... Val Loss: 1.3471\n",
            "Epoch: 18/20... Step: 1910... Loss: 1.3576... Val Loss: 1.3460\n",
            "Epoch: 18/20... Step: 1920... Loss: 1.3643... Val Loss: 1.3457\n",
            "Epoch: 18/20... Step: 1930... Loss: 1.3178... Val Loss: 1.3441\n",
            "Epoch: 18/20... Step: 1940... Loss: 1.3801... Val Loss: 1.3430\n",
            "Epoch: 19/20... Step: 1950... Loss: 1.3510... Val Loss: 1.3423\n",
            "Epoch: 19/20... Step: 1960... Loss: 1.3835... Val Loss: 1.3416\n",
            "Epoch: 19/20... Step: 1970... Loss: 1.3618... Val Loss: 1.3407\n",
            "Epoch: 19/20... Step: 1980... Loss: 1.3410... Val Loss: 1.3382\n",
            "Epoch: 19/20... Step: 1990... Loss: 1.3352... Val Loss: 1.3401\n",
            "Epoch: 19/20... Step: 2000... Loss: 1.3529... Val Loss: 1.3386\n",
            "Epoch: 19/20... Step: 2010... Loss: 1.3659... Val Loss: 1.3365\n",
            "Epoch: 19/20... Step: 2020... Loss: 1.3703... Val Loss: 1.3352\n",
            "Epoch: 19/20... Step: 2030... Loss: 1.3501... Val Loss: 1.3333\n",
            "Epoch: 19/20... Step: 2040... Loss: 1.3347... Val Loss: 1.3351\n",
            "Epoch: 19/20... Step: 2050... Loss: 1.3704... Val Loss: 1.3315\n",
            "Epoch: 20/20... Step: 2060... Loss: 1.3466... Val Loss: 1.3300\n",
            "Epoch: 20/20... Step: 2070... Loss: 1.3342... Val Loss: 1.3338\n",
            "Epoch: 20/20... Step: 2080... Loss: 1.3402... Val Loss: 1.3280\n",
            "Epoch: 20/20... Step: 2090... Loss: 1.3389... Val Loss: 1.3300\n",
            "Epoch: 20/20... Step: 2100... Loss: 1.3286... Val Loss: 1.3274\n",
            "Epoch: 20/20... Step: 2110... Loss: 1.3000... Val Loss: 1.3289\n",
            "Epoch: 20/20... Step: 2120... Loss: 1.3173... Val Loss: 1.3268\n",
            "Epoch: 20/20... Step: 2130... Loss: 1.3663... Val Loss: 1.3264\n",
            "Epoch: 20/20... Step: 2140... Loss: 1.3471... Val Loss: 1.3248\n",
            "Epoch: 20/20... Step: 2150... Loss: 1.3065... Val Loss: 1.3237\n",
            "Epoch: 20/20... Step: 2160... Loss: 1.3471... Val Loss: 1.3241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKlx9__YkS0d"
      },
      "source": [
        "**TESTING WITH THE ONLINE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJYa7Y2ugFW1",
        "outputId": "a17a3f00-32c2-4f1b-8a1c-c51d2a47cc6d"
      },
      "source": [
        "lstm.eval()\n",
        "import torch.nn.functional as F\n",
        "prime = 'Anna'\n",
        "\n",
        "# TO GIVE A PROPER START TO THE PREDICTION\n",
        "doc = [ch for ch in prime]\n",
        "\n",
        "h = lstm.init_hidden(1)\n",
        "\n",
        "TOP_K = 5\n",
        "\n",
        "def prediction(dataloader, lstm, character, h):\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    character = np.array([[dataloader.C2I[character]]])\n",
        "    character = dataloader.one_hot(character, dataloader.n_characters)\n",
        "    character = T.Tensor(character)\n",
        "\n",
        "    character = character.to('cuda')\n",
        "\n",
        "    out, h = lstm(character, h)\n",
        "\n",
        "    prob = F.softmax(out, dim = 1).data\n",
        "\n",
        "    prob, top_ch = prob.topk(TOP_K)\n",
        "    top_ch, prob = top_ch.cpu().numpy().squeeze(), prob.cpu().numpy().squeeze()\n",
        "\n",
        "    char = np.random.choice(top_ch, p = prob/prob.sum())\n",
        "\n",
        "    return dataloader.I2C[char], h\n",
        "\n",
        "for ch in doc:\n",
        "    char, h = prediction(data_loader, lstm, ch, h)\n",
        "\n",
        "doc.append(char)\n",
        "\n",
        "for i in range(1000):\n",
        "    char, h = prediction(data_loader, lstm, doc[-1], h)\n",
        "    doc.append(char)\n",
        "\n",
        "print(''.join(doc))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna Alexendrovna's later wished to say\n",
            "and would have to succe divines, with the country of his.\n",
            "\n",
            "\"I'm not thinking of the rain of a son well, that\n",
            "you were about their perflections\n",
            "of the sound that have a\n",
            "possibility of\n",
            "his\n",
            "wife, to brought them with her sincery, what in the wead as some work.\n",
            "\n",
            "\"You know,\" answered Levin, gave him something\n",
            "something the sat down.\n",
            "\n",
            "\"Well, as to discrest man in the means to get to be, and I should be as a little,\" said Levin.\n",
            "\n",
            "\"Oh, I have think I will not talk of to have to be a significance to be say, a mind to supposing you there was to say.\n",
            "\n",
            "\"I was\n",
            "a pretty was solether to the\n",
            "religion they, and\n",
            "that I\n",
            "could so say ill in a self-girls. But was the famely confinited the certain of\n",
            "thichif tears when you know why is their same the moment\n",
            "wourd a stear and the princess and something, I've come\n",
            "into the sand that I can great this feelings of all other, would have\n",
            "soliced as a present mild and she have both there's now it went to me to the same hunor and th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKAiLVDfkLSY"
      },
      "source": [
        "**LOADING THE MODEL WITH THE STATE-DICT AND TESTING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8PeSlFZhpb3",
        "outputId": "8cfad798-7d22-4905-f7e6-919722841449"
      },
      "source": [
        "lstm = LSTM(data_loader.n_characters, 512, 2, data_loader.n_characters)\n",
        "\n",
        "chkpt = T.load(r'/content/drive/MyDrive/CHARACTER_LEVEL_RNN/checkpoint.pth')\n",
        "lstm.load_state_dict(chkpt)\n",
        "lstm.to('cuda')\n",
        "lstm.eval()\n",
        "\n",
        "import torch.nn.functional as F\n",
        "prime = 'Anna'\n",
        "\n",
        "# TO GIVE A PROPER START TO THE PREDICTION\n",
        "doc = [ch for ch in prime]\n",
        "\n",
        "h = lstm.init_hidden(1)\n",
        "\n",
        "TOP_K = 5\n",
        "\n",
        "def prediction(dataloader, lstm, character, h):\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    character = np.array([[dataloader.C2I[character]]])\n",
        "    character = dataloader.one_hot(character, dataloader.n_characters)\n",
        "    character = T.Tensor(character)\n",
        "\n",
        "    character = character.to('cuda')\n",
        "\n",
        "    out, h = lstm(character, h)\n",
        "\n",
        "    prob = F.softmax(out, dim = 1).data\n",
        "\n",
        "    prob, top_ch = prob.topk(TOP_K)\n",
        "    top_ch, prob = top_ch.cpu().numpy().squeeze(), prob.cpu().numpy().squeeze()\n",
        "\n",
        "    char = np.random.choice(top_ch, p = prob/prob.sum())\n",
        "\n",
        "    return dataloader.I2C[char], h\n",
        "\n",
        "for ch in doc:\n",
        "    char, h = prediction(data_loader, lstm, ch, h)\n",
        "\n",
        "doc.append(char)\n",
        "\n",
        "for i in range(1000):\n",
        "    char, h = prediction(data_loader, lstm, doc[-1], h)\n",
        "    doc.append(char)\n",
        "\n",
        "print(''.join(doc))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna it serioully corround, but she droused her, and he\n",
            "could be\n",
            "dispositible, and seemed.\n",
            "\n",
            "\"I should have a mature, that I want to be insult. It's so\n",
            "matter at that\n",
            "secration of her\n",
            "musch, and would never be considering that it was stort, too must\n",
            "hissout and sething tears. Why she\n",
            "had to see them.\n",
            "What do\n",
            "you ask you the porstion\n",
            "made the strusgle of it?\" the doctor's smile was talking of the door.\n",
            "\n",
            "\"Yom to\n",
            "go to something with you,\" said Varenka in said of\n",
            "a finseros of the presence.\n",
            "\n",
            "\"What shall I should not be sure and what you do not ask me,\" said\n",
            "Levin, smiling, \"I want to see her arms to the master.\"\n",
            "\n",
            "\"Why, it were that you was to be such as a suffer of tender short for a listling for that man's answer and the carriage, but yes tell her husband and was now he had been an interest it asseed, but\n",
            "and there's the convisted of sorry with the contorried, there is so sereat in the same socint,\n",
            "why have not but a lot on the consequence.\"\n",
            "\n",
            "\"If you do you were!\" he asked, graying smile. He c\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}